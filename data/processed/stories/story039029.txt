Using your fingerprint to identify yourself seems beguilingly simple: it belongs only to you, and you aren't going to lose it. Apple's use of fingerprint technology -- although not the first in the industry -- seems very in tune with its ethos of making devices easy to use.
However, how safe fingerprint technologies really are does depend on how they are implemented. You might ask, is my fingerprint stored, who else can access it? Can the government demand that Apple hand my fingerprints to them, or use Apple to identify criminals from their database?
Apple have stated on record that they do not store fingerprints, and nor does the device. Instead, the iPhone stores the result of a check -- a "hash," which may be unique, but can't reveal your fingerprint.
But after the revelations by Edward Snowden that Apple was one of four Internet companies to have handed over data to the NSA, Apple has a much harder job to persuade the public to trust them. We need to know that backdoors won't be built into iPhones to allow security services' to retrieve your fingerprint data. The NSA has been shown to have a program of demanding means of entry to the software and hardware of all cell phones, through introducing security flaws. These flaws -- software bugs -- are also available to criminals and competing security agencies to exploit. Are we going to trust Apple's security won't be compromised? They are participants in the PRISM scheme -- whatever precisely that involves. How else are they co-operating with secret NSA demands?
The long-term answer to these security trust issues is to reveal the whole of these systems, including the underlying "source code" so that computer programmers can check how they work, and ensure that systems are not compromised. That's not really the kind of approach that Apple has been famous for, often being very closed about their software development and asking us to trust them to know best. Citizens and businesses should not have to trust their security to systems they cannot examine.
Apple's fingerprint system does create a risk of "normalizing" biometrics. Because of the iPhone's widespread use, people may increasingly expect to use similar "easy" and "safe" biometric systems, without considering that they create highly personal identification, with risks of being tracked and surveilled, and yet do not necessarily deliver the security that they imply. Other low level fingerprint ID systems suffer the same problems, yet are being employed in schools, even to replace library cards.
Biometrics can create a false sense of security. It is easy to assume that the tools really are a hard identification of an individual, and therefore, the technology cannot be fooled, or go wrong. However, systems can be fooled. Fingerprints and even your iris can be replicated.
Apple's fingerprint system may encounter a simple problem, in that the key to unlocking your phone -- your fingerprint -- could well be liberally scattered across the phone you are trying to protect. While the phone may also look for body heat, or skin irregularities, there is at least a distinct path which could be used to try to break into a phone. We'll have to see if anyone can use it successfully.
Some of these risks are manageable, through transparency and audit. The wider social risks are far harder to manage; arguably they aren't Apple's problem. Perhaps society needs to start having a much more intense debate about security and privacy, starting off with asking: whose security are we worrying about: my own, or the state's? How do I know what risks I am taking, and why should I trust any of the claims that are made?
People understand what a wallet or a doorlock is. The risks with personal and financial information are less tangible, and the risks of state abuse of power sometimes less tangible still. But these come with the digital technology; we will truly be citizens of the digital age when we can successfully debate and deal with these problems.
The opinions expressed in this commentary are solely those of Jim Killock.
